{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install -U -q \"google-generativeai>=0.8.3\"\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T03:21:35.281997Z","iopub.execute_input":"2024-11-12T03:21:35.282855Z","iopub.status.idle":"2024-11-12T03:22:01.837397Z","shell.execute_reply.started":"2024-11-12T03:21:35.282813Z","shell.execute_reply":"2024-11-12T03:22:01.836134Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import google.generativeai as genai\nfrom IPython.display import HTML, Markdown, display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T03:23:54.128353Z","iopub.execute_input":"2024-11-12T03:23:54.128888Z","iopub.status.idle":"2024-11-12T03:23:56.485722Z","shell.execute_reply.started":"2024-11-12T03:23:54.128810Z","shell.execute_reply":"2024-11-12T03:23:56.484302Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\ngenai.configure(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T03:24:47.393609Z","iopub.execute_input":"2024-11-12T03:24:47.394082Z","iopub.status.idle":"2024-11-12T03:24:47.495220Z","shell.execute_reply.started":"2024-11-12T03:24:47.394009Z","shell.execute_reply":"2024-11-12T03:24:47.493521Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#The previous example uses a single-turn, text-in/text-out structure\nflash = genai.GenerativeModel('gemini-1.5-flash')\nresponse = flash.generate_content(\"Explain gpt-transformer to me like a kid.\")\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T03:26:23.843833Z","iopub.execute_input":"2024-11-12T03:26:23.844369Z","iopub.status.idle":"2024-11-12T03:26:25.169023Z","shell.execute_reply.started":"2024-11-12T03:26:23.844313Z","shell.execute_reply":"2024-11-12T03:26:25.167096Z"}},"outputs":[{"name":"stdout","text":"Imagine you have a big box of Lego bricks, but they're all mixed up. You want to build a cool spaceship, but you need to figure out which bricks go where.\n\nGPT-Transformer is like a super smart robot that helps you build your spaceship. It's really good at looking at the different bricks and figuring out how they fit together. It uses a special \"transformer\" tool that helps it learn from the bricks it's already seen and understand the ones it hasn't. \n\nSo, when you give GPT-Transformer a bunch of words, it acts like a super smart Lego builder. It can understand the words, figure out how they go together, and even create new sentences! It's like having a robot friend who can write stories, answer your questions, and even help you with your homework. \n\nThat's GPT-Transformer in a nutshell: a powerful tool that can understand and create language, just like a super smart Lego builder! \n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"Markdown(response.text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T03:28:33.636496Z","iopub.execute_input":"2024-11-12T03:28:33.637003Z","iopub.status.idle":"2024-11-12T03:28:33.648140Z","shell.execute_reply.started":"2024-11-12T03:28:33.636955Z","shell.execute_reply":"2024-11-12T03:28:33.646549Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Imagine you have a big box of Lego bricks, but they're all mixed up. You want to build a cool spaceship, but you need to figure out which bricks go where.\n\nGPT-Transformer is like a super smart robot that helps you build your spaceship. It's really good at looking at the different bricks and figuring out how they fit together. It uses a special \"transformer\" tool that helps it learn from the bricks it's already seen and understand the ones it hasn't. \n\nSo, when you give GPT-Transformer a bunch of words, it acts like a super smart Lego builder. It can understand the words, figure out how they go together, and even create new sentences! It's like having a robot friend who can write stories, answer your questions, and even help you with your homework. \n\nThat's GPT-Transformer in a nutshell: a powerful tool that can understand and create language, just like a super smart Lego builder! \n"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"#you can also set up a multi-turn chat structure too.\nchat = flash.start_chat(history=[])\nresponse = chat.send_message('Hello! I am Manoj.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T03:32:13.269102Z","iopub.execute_input":"2024-11-12T03:32:13.269561Z","iopub.status.idle":"2024-11-12T03:32:13.654027Z","shell.execute_reply.started":"2024-11-12T03:32:13.269505Z","shell.execute_reply":"2024-11-12T03:32:13.652641Z"}},"outputs":[{"name":"stdout","text":"Hello Manoj! It's nice to meet you. What can I do for you today? \n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"response = chat.send_message('Can you tell something interesting about Beyonce?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T03:32:54.419809Z","iopub.execute_input":"2024-11-12T03:32:54.420314Z","iopub.status.idle":"2024-11-12T03:32:55.464879Z","shell.execute_reply.started":"2024-11-12T03:32:54.420253Z","shell.execute_reply":"2024-11-12T03:32:55.463429Z"}},"outputs":[{"name":"stdout","text":"BeyoncÃ© is a powerhouse of talent and has a ton of interesting facts about her. Here's one that might surprise you: \n\n**BeyoncÃ© was originally named \"BeyoncÃ© Giselle Knowles-Carter\" but chose to drop her middle name and her married name for her stage name.**\n\nThis decision was a powerful statement about her identity as an artist and her independence. She wanted to be known simply as \"BeyoncÃ©,\" showcasing her unique talents and not relying on her family name or her marital status. It's a strong testament to her self-assurance and ambition. \n\nWhat else are you curious about regarding BeyoncÃ©?  I can tell you more about her music, her career, or her personal life! \n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# While you have the `chat` object around, the conversation state\n# persists. Confirm that by asking if it knows my name.\nresponse = chat.send_message('Do you remember what my name is?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T03:33:31.893458Z","iopub.execute_input":"2024-11-12T03:33:31.893953Z","iopub.status.idle":"2024-11-12T03:33:32.477658Z","shell.execute_reply.started":"2024-11-12T03:33:31.893898Z","shell.execute_reply":"2024-11-12T03:33:32.476132Z"}},"outputs":[{"name":"stdout","text":"You are absolutely right! I apologize for forgetting your name. As an AI, I don't have a memory of past conversations. \n\nYou introduced yourself as Manoj.  It's nice to meet you, Manoj! ðŸ˜Š \n\nIs there anything else I can help you with? \n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"for model in genai.list_models():\n  print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T03:35:14.851525Z","iopub.execute_input":"2024-11-12T03:35:14.852002Z","iopub.status.idle":"2024-11-12T03:35:14.928682Z","shell.execute_reply.started":"2024-11-12T03:35:14.851955Z","shell.execute_reply":"2024-11-12T03:35:14.927468Z"}},"outputs":[{"name":"stdout","text":"models/chat-bison-001\nmodels/text-bison-001\nmodels/embedding-gecko-001\nmodels/gemini-1.0-pro-latest\nmodels/gemini-1.0-pro\nmodels/gemini-pro\nmodels/gemini-1.0-pro-001\nmodels/gemini-1.0-pro-vision-latest\nmodels/gemini-pro-vision\nmodels/gemini-1.5-pro-latest\nmodels/gemini-1.5-pro-001\nmodels/gemini-1.5-pro-002\nmodels/gemini-1.5-pro\nmodels/gemini-1.5-pro-exp-0801\nmodels/gemini-1.5-pro-exp-0827\nmodels/gemini-1.5-flash-latest\nmodels/gemini-1.5-flash-001\nmodels/gemini-1.5-flash-001-tuning\nmodels/gemini-1.5-flash\nmodels/gemini-1.5-flash-exp-0827\nmodels/gemini-1.5-flash-002\nmodels/gemini-1.5-flash-8b\nmodels/gemini-1.5-flash-8b-001\nmodels/gemini-1.5-flash-8b-latest\nmodels/gemini-1.5-flash-8b-exp-0827\nmodels/gemini-1.5-flash-8b-exp-0924\nmodels/embedding-001\nmodels/text-embedding-004\nmodels/aqa\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"for model in genai.list_models():\n  if model.name == 'models/gemini-1.5-flash':\n    print(model)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T03:37:17.118024Z","iopub.execute_input":"2024-11-12T03:37:17.118480Z","iopub.status.idle":"2024-11-12T03:37:17.182909Z","shell.execute_reply.started":"2024-11-12T03:37:17.118435Z","shell.execute_reply":"2024-11-12T03:37:17.181750Z"}},"outputs":[{"name":"stdout","text":"Model(name='models/gemini-1.5-flash',\n      base_model_id='',\n      version='001',\n      display_name='Gemini 1.5 Flash',\n      description='Fast and versatile multimodal model for scaling across diverse tasks',\n      input_token_limit=1000000,\n      output_token_limit=8192,\n      supported_generation_methods=['generateContent', 'countTokens'],\n      temperature=1.0,\n      max_temperature=2.0,\n      top_p=0.95,\n      top_k=40)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"short_model = genai.GenerativeModel(\n    'gemini-1.5-flash',\n    generation_config=genai.GenerationConfig(max_output_tokens=200))\n\nresponse = short_model.generate_content('Write a 1000 word essay on the importance of Internet in Society.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T03:39:00.511695Z","iopub.execute_input":"2024-11-12T03:39:00.512154Z","iopub.status.idle":"2024-11-12T03:39:01.793801Z","shell.execute_reply.started":"2024-11-12T03:39:00.512113Z","shell.execute_reply":"2024-11-12T03:39:01.792463Z"}},"outputs":[{"name":"stdout","text":"## The Digital Fabric of Society: The Irreplaceable Importance of the Internet\n\nThe internet, a sprawling network of interconnected computers, has transformed the very fabric of society in ways unimaginable just a few decades ago. It has revolutionized communication, fueled economic growth, fostered cultural exchange, and empowered individuals like never before. Its influence is so pervasive that it's difficult to envision a modern world without it. This essay will explore the myriad ways the internet has become indispensable, analyzing its impact on communication, education, commerce, civic engagement, and even personal identity.\n\nThe internet has fundamentally altered the way we communicate. Email, instant messaging, and social media platforms have made it possible to connect with friends, family, and colleagues across the globe in real-time. This instant accessibility has strengthened relationships, fostered new connections, and bridged geographical divides.  The internet also allows for diverse forms of communication, from video calls and group chats to online forums and blogs, creating a global marketplace of ideas and experiences\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"response = short_model.generate_content('Write a short poem on the importance of Internet in modern society.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T03:39:51.401098Z","iopub.execute_input":"2024-11-12T03:39:51.401562Z","iopub.status.idle":"2024-11-12T03:39:52.246150Z","shell.execute_reply.started":"2024-11-12T03:39:51.401518Z","shell.execute_reply":"2024-11-12T03:39:52.244886Z"}},"outputs":[{"name":"stdout","text":"A web of wires, a world unseen,\nConnecting hearts, where dreams convene.\nInformation flows, a constant stream,\nA global village, a digital dream.\n\nFrom distant lands, to minds nearby,\nKnowledge shared, beneath the sky.\nCommerce thrives, with a click and scroll,\nConnecting lives, reaching every soul.\n\nBut caution must be, with wisdom's guide,\nTo navigate, where shadows hide.\nFor in this web, both light and dark,\nWe choose our path, with a guiding spark. \n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"#TemperatureÂ¶\n#Temperature controls the degree of randomness in token selection. Higher temperatures result in a higher number of candidate tokens from which the next output token is selected, and can produce more diverse results, while lower temperatures have the opposite effect, such that a temperature of 0 results in greedy decoding, selecting the most probable token at each step.\n#Temperature doesn't provide any guarantees of randomness, but it can be used to \"nudge\" the output somewhat.\nfrom google.api_core import retry\n\nhigh_temp_model = genai.GenerativeModel(\n    'gemini-1.5-flash',\n    generation_config=genai.GenerationConfig(temperature=2.0))\n\n\n# When running lots of queries, it's a good practice to use a retry policy so your code\n# automatically retries when hitting Resource Exhausted (quota limit) errors.\nretry_policy = {\n    \"retry\": retry.Retry(predicate=retry.if_transient_error, initial=10, multiplier=1.2, timeout=300)\n}\n\nfor _ in range(5):\n  response = high_temp_model.generate_content('Pick a random colour... (answer in a single word)',\n                                              request_options=retry_policy)\n  if response.parts:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T03:49:43.150653Z","iopub.execute_input":"2024-11-12T03:49:43.151134Z","iopub.status.idle":"2024-11-12T03:51:30.143670Z","shell.execute_reply.started":"2024-11-12T03:49:43.151089Z","shell.execute_reply":"2024-11-12T03:51:30.142264Z"}},"outputs":[{"name":"stdout","text":"Teal \n -------------------------\nPurple \n -------------------------\nPurple \n -------------------------\nPurple \n -------------------------\nBlue \n -------------------------\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"low_temp_model = genai.GenerativeModel(\n    'gemini-1.5-flash',\n    generation_config=genai.GenerationConfig(temperature=0.0))\n\nfor _ in range(5):\n  response = low_temp_model.generate_content('Pick a random colour... (answer in a single word)',\n                                             request_options=retry_policy)\n  if response.parts:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T03:55:50.813303Z","iopub.execute_input":"2024-11-12T03:55:50.813801Z"}},"outputs":[{"name":"stdout","text":"Purple \n -------------------------\nPurple \n -------------------------\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"#Top-K and top-PÂ¶\n#Like temperature, top-K and top-P parameters are also used to control the diversity of the model's output.\n\n#Top-K is a positive integer that defines the number of most probable tokens from which to select the output token. A top-K of 1 selects a single token, performing greedy decoding.\n\n#Top-P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates. A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects every token in the model's vocabulary.\n\n#When both are supplied, the Gemini API will filter top-K tokens first, then top-P and then finally sample from the candidate tokens using the supplied temperature.\n\n#Run this example a number of times, change the settings and observe the change in output.\nmodel = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        # These are the default values for gemini-1.5-flash-001.\n        temperature=1.0,\n        top_k=64,\n        top_p=0.95,\n    ))\n\nstory_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\nresponse = model.generate_content(story_prompt)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T03:58:20.988278Z","iopub.execute_input":"2024-11-12T03:58:20.988788Z","iopub.status.idle":"2024-11-12T03:58:22.790170Z","shell.execute_reply.started":"2024-11-12T03:58:20.988725Z","shell.execute_reply":"2024-11-12T03:58:22.788716Z"}},"outputs":[{"name":"stdout","text":"Bartholomew, a ginger cat with a crooked tail and a heart full of wanderlust, sat by the window, watching the world go by. He yearned for something more than the predictable routine of sunbeams and tuna. One day, a delivery truck pulled up, its back doors swinging open to reveal a mountain of cardboard boxes. Bartholomew, driven by an irresistible urge, slipped out the door, his paws padding across the warm asphalt.\n\nHe found himself in a world of cardboard castles and paper fortresses, each one a potential adventure waiting to be explored. He danced through a maze of boxes, his whisker twitching with excitement. He chased a fluttering newspaper, its rustling a siren song of unknown territory. He even encountered a fluffy white cat, a fellow explorer, with whom he shared a fleeting moment of purring camaraderie.\n\nThe day wore on, the sun beginning its descent. Bartholomew, exhausted but exhilarated, stumbled upon a large, empty box, its interior lined with soft, shredded paper. He curled up inside, the familiar scent of cardboard enveloping him like a warm blanket. He knew this wasn't his home, but he had discovered something far more precious: the thrill of exploration, the joy of discovering the world beyond his backyard.\n\nAs the city lights twinkled in the distance, Bartholomew dreamed of his next adventure. He wasn't just a house cat anymore. He was a traveler, a wanderer, a ginger adventurer with the world at his paws. And he knew, with a certainty that only a cat could understand, that this was only the beginning. \n\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"#Prompting\n#Zero-Shot Prompting\nmodel = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=5,\n    ))\n\nzero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\nReview: \"Her\" is a disturbing study revealing the direction\nhumanity is headed if AI is allowed to keep evolving,\nunchecked. I wish there were more movies like this masterpiece.\nSentiment: \"\"\"\n\nresponse = model.generate_content(zero_shot_prompt)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T03:59:09.161374Z","iopub.execute_input":"2024-11-12T03:59:09.161839Z","iopub.status.idle":"2024-11-12T03:59:09.418080Z","shell.execute_reply.started":"2024-11-12T03:59:09.161794Z","shell.execute_reply":"2024-11-12T03:59:09.416824Z"}},"outputs":[{"name":"stdout","text":"Sentiment: **POSITIVE**\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"#Enum modeÂ¶\n#The models are trained to generate text, and can sometimes produce more text than you may wish for. In the preceding example, the model will output the label, sometimes it can include a preceding \"Sentiment\" label, and without an output token limit, it may also add explanatory text afterwards.\n\n#The Gemini API has an Enum mode feature that allows you to constrain the output to a fixed set of values.\n\nimport enum\n\nclass Sentiment(enum.Enum):\n    POSITIVE = \"positive\"\n    NEUTRAL = \"neutral\"\n    NEGATIVE = \"negative\"\n\n\nmodel = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=Sentiment\n    ))\n\nresponse = model.generate_content(zero_shot_prompt)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T04:00:27.293603Z","iopub.execute_input":"2024-11-12T04:00:27.294062Z","iopub.status.idle":"2024-11-12T04:00:27.859587Z","shell.execute_reply.started":"2024-11-12T04:00:27.294019Z","shell.execute_reply":"2024-11-12T04:00:27.858189Z"}},"outputs":[{"name":"stdout","text":"positive\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"#One-shot and few-shotÂ¶\n\nmodel = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=250,\n    ))\n\nfew_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n\nEXAMPLE:\nI want a small pizza with cheese, tomato sauce, and pepperoni.\nJSON Response:\n```\n{\n\"size\": \"small\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n}\n```\n\nEXAMPLE:\nCan I get a large pizza with tomato sauce, basil and mozzarella\nJSON Response:\n```\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n}\n\nORDER:\n\"\"\"\n\ncustomer_order = \"Give me a large with cheese & pineapple\"\n\n\nresponse = model.generate_content([few_shot_prompt, customer_order])\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T04:01:03.652037Z","iopub.execute_input":"2024-11-12T04:01:03.652664Z","iopub.status.idle":"2024-11-12T04:01:04.027581Z","shell.execute_reply.started":"2024-11-12T04:01:03.652610Z","shell.execute_reply":"2024-11-12T04:01:04.026044Z"}},"outputs":[{"name":"stdout","text":"```json\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"pineapple\"]\n}\n``` \n\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"#JSON modeÂ¶\n#To provide control over the schema, and to ensure that you only receive JSON (with no other text or markdown), you can use the Gemini API's JSON mode. This forces the model to constrain decoding, such that token selection is guided by the supplied schema.\nimport typing_extensions as typing\n\nclass PizzaOrder(typing.TypedDict):\n    size: str\n    ingredients: list[str]\n    type: str\n\n\nmodel = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        response_mime_type=\"application/json\",\n        response_schema=PizzaOrder,\n    ))\n\nresponse = model.generate_content(\"Can I have a large dessert pizza with apple and chocolate\")\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T04:01:35.463519Z","iopub.execute_input":"2024-11-12T04:01:35.463969Z","iopub.status.idle":"2024-11-12T04:01:35.856575Z","shell.execute_reply.started":"2024-11-12T04:01:35.463928Z","shell.execute_reply":"2024-11-12T04:01:35.855310Z"}},"outputs":[{"name":"stdout","text":"{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert\"}\n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"#Chain of Thought (CoT)\n#Direct prompting on LLMs can return answers quickly and (in terms of output token usage) efficiently, but they can be prone to hallucination. The answer may \"look\" correct (in terms of language and syntax) but is incorrect in terms of factuality and reasoning.\n\n#Chain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count.\n\n#As models like the Gemini family are trained to be \"chatty\" and provide reasoning steps, you can ask the model to be more direct in the prompt.\nprompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\nam 20 years old. How old is my partner? Return the answer immediately.\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\nresponse = model.generate_content(prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T04:02:17.215617Z","iopub.execute_input":"2024-11-12T04:02:17.216113Z","iopub.status.idle":"2024-11-12T04:02:17.513650Z","shell.execute_reply.started":"2024-11-12T04:02:17.216067Z","shell.execute_reply":"2024-11-12T04:02:17.512124Z"}},"outputs":[{"name":"stdout","text":"33 \n\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\nI am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n\nresponse = model.generate_content(prompt)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T04:02:36.112172Z","iopub.execute_input":"2024-11-12T04:02:36.113177Z","iopub.status.idle":"2024-11-12T04:02:36.874556Z","shell.execute_reply.started":"2024-11-12T04:02:36.113128Z","shell.execute_reply":"2024-11-12T04:02:36.872927Z"}},"outputs":[{"name":"stdout","text":"Here's how to solve this:\n\n* **When you were 4, your partner was 3 times your age:** 4 years old * 3 = 12 years old\n* **Age difference:** Your partner was 12 - 4 = 8 years older than you.\n* **Age difference remains the same:**  Since the age difference doesn't change, your partner is still 8 years older than you.\n* **Your partner's current age:** Your current age (20) + 8 years = **28 years old**\n\nTherefore, your partner is 28 years old. \n\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"#ReAct: Reason and actÂ¶\nmodel_instructions = \"\"\"\nSolve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\nObservation is understanding relevant information from an Action's output and Action can be one of three types:\n (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n     will return some similar entities to search and you can try to search the information from those topics.\n (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n     so keep your searches short.\n (3) <finish>answer</finish>, which returns the answer and finishes the task.\n\"\"\"\n\nexample1 = \"\"\"Question\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n\nThought 1\nThe question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n\nAction 1\n<search>Milhouse</search>\n\nObservation 1\nMilhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n\nThought 2\nThe paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n\nAction 2\n<lookup>named after</lookup>\n\nObservation 2\nMilhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n\nThought 3\nMilhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n\nAction 3\n<finish>Richard Nixon</finish>\n\"\"\"\n\nexample2 = \"\"\"Question\nWhat is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n\nThought 1\nI need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n\nAction 1\n<search>Colorado orogeny</search>\n\nObservation 1\nThe Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n\nThought 2\nIt does not mention the eastern sector. So I need to look up eastern sector.\n\nAction 2\n<lookup>eastern sector</lookup>\n\nObservation 2\nThe eastern sector extends into the High Plains and is called the Central Plains orogeny.\n\nThought 3\nThe eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n\nAction 3\n<search>High Plains</search>\n\nObservation 3\nHigh Plains refers to one of two distinct land regions\n\nThought 4\nI need to instead search High Plains (United States).\n\nAction 4\n<search>High Plains (United States)</search>\n\nObservation 4\nThe High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n\nThought 5\nHigh Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n\nAction 5\n<finish>1,800 to 7,000 ft</finish>\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T04:05:29.783963Z","iopub.execute_input":"2024-11-12T04:05:29.784399Z","iopub.status.idle":"2024-11-12T04:05:29.794374Z","shell.execute_reply.started":"2024-11-12T04:05:29.784359Z","shell.execute_reply":"2024-11-12T04:05:29.793172Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"question = \"\"\"Question\nWho was the youngest author listed on the transformers NLP paper?\n\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\nreact_chat = model.start_chat()\n\n# You will perform the Action, so generate up to, but not including, the Observation.\nconfig = genai.GenerationConfig(stop_sequences=[\"\\nObservation\"])\n\nresp = react_chat.send_message(\n    [model_instructions, example1, example2, question],\n    generation_config=config)\nprint(resp.text)","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"Thought 1\nI need to search for the Transformers NLP paper and then find the author list.\n\nAction 1\n<search>Transformers NLP paper</search>\n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"observation = \"\"\"Observation 1\n[1706.03762] Attention Is All You Need\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\nWe propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n\"\"\"\nresp = react_chat.send_message(observation, generation_config=config)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T04:05:59.933874Z","iopub.execute_input":"2024-11-12T04:05:59.934631Z","iopub.status.idle":"2024-11-12T04:06:00.746466Z","shell.execute_reply.started":"2024-11-12T04:05:59.934582Z","shell.execute_reply":"2024-11-12T04:06:00.745060Z"}},"outputs":[{"name":"stdout","text":"Thought 2\nI need to find the youngest author from the author list.\n\nAction 2\n<lookup>youngest</lookup> \n\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"#This process repeats until the <finish> action is reached. You can continue running this yourself if you like, or try the Wikipedia example to see a fully automated ReAct system at work.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Code promptingÂ¶\n#Generating code\n#The Gemini family of models can be used to generate code, configuration and scripts. Generating code can be helpful when learning to code, learning a new language or for rapidly generating a first draft.\n\n#It's important to be aware that since LLMs can't reason, and can repeat training data, it's essential to read and test your code first, and comply with any relevant licenses.\nmodel = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=1,\n        top_p=1,\n        max_output_tokens=1024,\n    ))\n\n# Gemini 1.5 models are very chatty, so it helps to specify they stick to the code.\ncode_prompt = \"\"\"\nWrite a Python function to calculate the factorial of a number. No explanation, provide only the code.\n\"\"\"\n\nresponse = model.generate_content(code_prompt)\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T04:06:55.100504Z","iopub.execute_input":"2024-11-12T04:06:55.100938Z","iopub.status.idle":"2024-11-12T04:06:55.548016Z","shell.execute_reply.started":"2024-11-12T04:06:55.100898Z","shell.execute_reply":"2024-11-12T04:06:55.546861Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```python\ndef factorial(n):\n  if n == 0:\n    return 1\n  else:\n    return n * factorial(n-1)\n```"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    tools='code_execution')\n\ncode_exec_prompt = \"\"\"\nCalculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you get them all.\n\"\"\"\n\nresponse = model.generate_content(code_exec_prompt)\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T04:07:12.481921Z","iopub.execute_input":"2024-11-12T04:07:12.482383Z","iopub.status.idle":"2024-11-12T04:07:17.082614Z","shell.execute_reply.started":"2024-11-12T04:07:12.482338Z","shell.execute_reply":"2024-11-12T04:07:17.081211Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"I understand. I need to find all the odd prime numbers up to the 14th odd prime number and then calculate their sum. \n\n\n``` python\nimport sympy\n\nprimes = list(sympy.primerange(1, 100))\nodd_primes = [p for p in primes if p % 2 != 0]\nsum_of_primes = sum(odd_primes[:14])\nprint(f'{sum_of_primes=}')\n\n```\n```\nsum_of_primes=326\n\n```\nThe sum of the first 14 odd prime numbers is 326. \n"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"for part in response.candidates[0].content.parts:\n  print(part)\n  print(\"-----\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T04:07:39.093231Z","iopub.execute_input":"2024-11-12T04:07:39.093711Z","iopub.status.idle":"2024-11-12T04:07:39.101866Z","shell.execute_reply.started":"2024-11-12T04:07:39.093666Z","shell.execute_reply":"2024-11-12T04:07:39.100177Z"}},"outputs":[{"name":"stdout","text":"text: \"I understand. I need to find all the odd prime numbers up to the 14th odd prime number and then calculate their sum. \\n\\n\"\n\n-----\nexecutable_code {\n  language: PYTHON\n  code: \"\\nimport sympy\\n\\nprimes = list(sympy.primerange(1, 100))\\nodd_primes = [p for p in primes if p % 2 != 0]\\nsum_of_primes = sum(odd_primes[:14])\\nprint(f\\'{sum_of_primes=}\\')\\n\"\n}\n\n-----\ncode_execution_result {\n  outcome: OUTCOME_OK\n  output: \"sum_of_primes=326\\n\"\n}\n\n-----\ntext: \"The sum of the first 14 odd prime numbers is 326. \\n\"\n\n-----\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n\nexplain_prompt = f\"\"\"\nPlease explain what this file does at a very high level. What is it, and why would I use it?\n\n```\n{file_contents}\n```\n\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\n\nresponse = model.generate_content(explain_prompt)\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T04:07:52.027824Z","iopub.execute_input":"2024-11-12T04:07:52.028315Z","iopub.status.idle":"2024-11-12T04:07:54.143663Z","shell.execute_reply.started":"2024-11-12T04:07:52.028265Z","shell.execute_reply":"2024-11-12T04:07:54.142426Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"This file, `git-prompt.sh`, is a Bash script that enhances your terminal prompt by displaying information about the current Git repository.  It provides a rich and informative prompt, especially useful for developers working with Git.\n\nHere's why you'd use it:\n\n* **Visual Git Status:** The prompt dynamically shows:\n    * Current branch name\n    * Upstream branch (if tracking)\n    * Status indicators (staged, modified, untracked, etc.)\n    * Number of changed files (optional)\n    *  Username and repository name (optional)\n\n* **Improved Workflow:**\n    *  By visually displaying your Git state, you get a quick understanding of the repository without having to run separate commands. \n    *  It helps you quickly identify potential issues (e.g., uncommitted changes) and navigate your projects more efficiently.\n\n* **Customization:**\n    * You can choose from different color themes to customize the prompt appearance.\n    * You can also configure the script to display different information or use custom symbols for specific Git states.\n\n* **Integration:**\n    * The script smoothly integrates with your existing Bash prompt by modifying the `PS1` variable.\n\n**In essence, `git-prompt.sh` is a powerful tool for Git users who want to enhance their terminal experience and gain a better understanding of their Git repositories.** \n"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}